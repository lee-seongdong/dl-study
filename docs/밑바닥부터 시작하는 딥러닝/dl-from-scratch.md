# dl-from-scratch
*밑바닥부터 시작하는 딥러닝*

## 딥러닝 모델의 구조
- 모델 = n개의 계층
- 계층 = n개의 뉴런 -> 네트워크 크기
- 뉴런 = n개의 가중치
- 오차 역전파를 통해 가중치들을 update하는 것이 딥러닝 모델 학습

## 가중치의 초깃값
- 무작위로 초기화 해야한다.
    - 가중치가 대칭적(같거나 비례)으로 초기화되면 모든 뉴런이 동일하게 학습되어, 동일하게 동작함. 뉴런을 여러 개 두는 의미가 없어짐
- 활성화 값의 분포가 적절해야한다.
    - 활성화 값이 치우치게 분포되면, 기울기 소실이나 기울기 폭발 문제가 생김. 뉴런을 여러 개 두는 의미가 없어짐
- tanh, sigmoid 활성화 함수는 `xavier` 초깃값, ReLU는 `He` 초깃값이 권장된다.


## 오버피팅
훈련 데이터에 지나치게 적응되어, 그 외의 데이터에는 제대로 대응하지 못하는 문제

### 오버피팅이 발생하는 경우
- 매개변수가 많고 표현력이 높은 모델
- 훈련 데이터가 적음

### 오버피팅 해결방법
- 가중치 감소: 큰 가중치에 대해서는 큰 페널티를 부과하여 학습
- 드롭아웃: 뉴런을 임의로 제외하며 학습

## 배치 정규화
값이 고르게 분포되도록 조정하는 기법.  
데이터 분포가 평균 0, 분산 1이 되도록 정규화

### 배치 정규화 계층
활성화값이 적당히 분포되도록 조정하는 계층

### 배치 정규화의 필요성
- 학습을 빨리 진행할 수 있다.
- 가중치 초깃값에 크게 의존하지 않는다.
- 오버피팅을 억제한다.

## 하이퍼파라미터
### 하이퍼파라미터 종류
- 각 층의 뉴런 수
- 배치 크기
- 학습률
- 가중치 감소

### 하이퍼파라미터 찾기
`검증 데이터`로 하이퍼파라미터의 성능 평가.
> - 훈련 데이터: 매개변수 학습
> - 검증 데이터: 하이퍼파라미터 성능 평가
> - 시험 데이터: 신경망의 범용 성능 평가 (마지막에 한번만)

### 하이퍼파라미터 최적화 과정
1. 하이퍼파라미터 값의 범위 설정
2. 설정된 범위에서 하이퍼파라미터 값을 무작위 추출
3. 추출된 값으로 학습을 진행하고, 검증 데이터로 정확도 평가 (에폭은 작게)
4. 2~3단계를 특정 횟수(100회 등) 반복하며, 하이퍼파라미터의 범위를 좁혀나감

## CNN
이미지나 음성 등 형상을 가진 데이터를 이해할 수 있는 네트워크  
완전 연결계층으로만 이루어진 네트워크는 데이터의 형상이 무시되어, 이를 제대로 학습할 수 없음  

### CNN의 구조
1개 이상의 Conv-ReLU-(Pooling) 계층 조합으로 구성됨

### 합성곱 연산 (Conv)
- `입력 데이터`를 `필터(커널)`이 순회하며 `단일 곱셈-누산`을 진행
- `단일 곱셈-누산` 결과에 `편향(스칼라)`을 더해 `출력`

### 합성곱 크기
2차원
- 입력 크기: (H, W)
- 필터 크기: (FH, FW)
- 출력 크기: (OH, OW)

3차원, 필터 1개
- 입력 크기: (C, H, W) -> H, W에 채널 C까지 고려한 3차원 데이터
- 필터 크기: (C, FH, FW) -> 필터의 채널 크기는 입력의 채널과 동일
- 출력 크기: (1, OH, OW) -> 2차원 데이터

3차원, 필터 FN개
- 입력 크기: (C, H, W)
- 필터 크기: (FN, C, FH, FW)
- 편향: (FN, 1, 1)
- 출력 크기: (FN, OH, OW)

배치 처리
- 입력 크기: (N, C, H, W)
- 필터 크기: (FN, C, FH, FW)
- 편향: (FN, 1, 1)
- 출력 크기: (N, FN, OH, OW)

### 합성곱 계층의 출력 크기 계산
- 입력 크기: (H, W)
- 필터 크기: (FH, FW)
- 패딩: P
- 스트라이드: S
- 출력 크기: (OH, OW)

> 출력의 크기는 `패딩`과 `스트라이드`로 조절
> - 패딩: 입력 데이터 주변을 특정 값으로 채우는 기법 (출력 크기 커짐) -> 출력 크기가 1로 수렴하는것을 방지
> - 스트라이드: 필터가 이동하는 간격을 설정하는 기법 (출력 크기 작아짐)
> - OH = (H + 2P - FH) / S + 1  
> - OW = (W + 2P - FW) / S + 1

### 풀링 계층 (Pooling)
데이터의 크기를 줄이는 연산 기법
- 풀링 윈도우의 크기와 스트라이드는 같은값으로 설정하는것을 권장(윈도우가 겹치지 않도록)
- 여러 풀링방법이 있지만, 일반적으로 `최대풀링`을 사용

### 풀링 계층의 특징
- 학습할 매개변수가 없음
- 데이터 채널의 변화를 주지 않음
- 입력 데이터 변화에 영향을 적게 받음



