# 학습 기법
`어떤 문제 -> 문제 해결 접근 -> 해결법` 으로 정리

## 가중치의 초깃값 최적화
- 무작위로 초기화 해야한다.
    - 가중치가 대칭적(같거나 비례)으로 초기화되면 모든 뉴런이 동일하게 학습되어, 동일하게 동작함. 뉴런을 여러 개 두는 의미가 없어짐
- 활성화 값의 분포가 적절해야한다.
    - 활성화 값이 치우치게 분포되면, 기울기 소실이나 기울기 폭발 문제가 생김. 뉴런을 여러 개 두는 의미가 없어짐
- tanh, sigmoid 활성화 함수는 `xavier` 초깃값, ReLU는 `He` 초깃값이 권장된다.


## 오버피팅
훈련 데이터에 지나치게 적응되어, 그 외의 데이터에는 제대로 대응하지 못하는 문제

### 오버피팅이 발생하는 경우
- 매개변수가 많고 표현력이 높은 모델
- 훈련 데이터가 적음

### 오버피팅 해결방법
- 가중치 감소: 큰 가중치에 대해서는 큰 페널티를 부과하여 학습
- 드롭아웃: 뉴런을 임의로 제외하며 학습

## 배치 정규화
값이 고르게 분포되도록 조정하는 기법.  
데이터 분포가 평균 0, 분산 1이 되도록 정규화

### 배치 정규화 계층
활성화값이 적당히 분포되도록 조정하는 계층

### 배치 정규화의 필요성
- 학습을 빨리 진행할 수 있다.
- 가중치 초깃값에 크게 의존하지 않는다.
- 오버피팅을 억제한다.